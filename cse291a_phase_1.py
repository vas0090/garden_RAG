# -*- coding: utf-8 -*-
"""cse291A_Phase_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EmrlllwkpErWTBP_LJMIVnkk4YWbrZWV

1. Data Preprocessing
"""

!pip install qdrant-client sentence-transformers pandas tqdm

!pip install numpy

pip install beautifulsoup4

# Imports
# Core libraries
import pandas as pd
import json
import numpy as np

# Sentence transformer for embeddings
from sentence_transformers import SentenceTransformer

# Qdrant client for vector database
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# Optional: show progress, handle warnings
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

# BeautifulSoup Import
from bs4 import BeautifulSoup

# --- Step 1: Load JSONL dataset ---
json_data = []
with open("dataset_plants_v5.jsonl", "r") as f:
    for line in f:
        json_data.append(json.loads(line))

df_json = pd.DataFrame(json_data)

print("JSONL Dataset Loaded")
print("Shape:", df_json.shape)
print("Columns:", df_json.columns.tolist())
print("\nSample rows from JSONL dataset:")
display(df_json.head(5))


# --- Step 2: Load Indoor Plant Health CSV ---
df_csv = pd.read_csv("Indoor_Plant_Health_and_Growth_Factors.csv")

print("\n Indoor Plant Health CSV Loaded")
print("Shape:", df_csv.shape)
print("Columns:", df_csv.columns.tolist())
print("\nSample rows from CSV dataset:")
display(df_csv.head(5))

# --- Clean HTML helper ---
def clean_html(text):
    if pd.isna(text):
        return ''
    return BeautifulSoup(str(text), 'html.parser').get_text(separator=' ', strip=True)


# --- Clean JSON columns (keep same logic) ---
for col in ['instruction', 'response']:
    if col in df_json.columns:
        df_json[col] = df_json[col].apply(clean_html)


# --- Create readable text from JSON dataset ---
df_json['formatted_text'] = "Q: " + df_json['instruction'].astype(str) + " A: " + df_json['response'].astype(str)


# --- Convert Indoor Plant CSV rows into natural language descriptions ---
def row_to_sentence(row):
    # Create a sentence describing all numerical + categorical info
    parts = []
    for col, val in row.items():
        if pd.notna(val):
            parts.append(f"{col.replace('_', ' ')} is {val}")
    return ". ".join(parts) + "."

# Apply this to every row in the CSV
df_csv['formatted_text'] = df_csv.apply(row_to_sentence, axis=1)


# --- Combine both datasets into a single text column ---
combined_df = pd.DataFrame({
    'text': pd.concat([df_json['formatted_text'], df_csv['formatted_text']], ignore_index=True)
})


# --- Save the result ---
combined_df.to_csv('combined_plants_dataset.csv', index=False)

print("Combined dataset created and saved!")
print("Shape:", combined_df.shape)
display(combined_df.sample(5))

"""2. Make Embeddings for our data"""

# --- Step 1: Load your prepared combined dataset ---
combined_df = pd.read_csv("combined_plants_dataset.csv")

print("Combined dataset loaded for embedding creation!")
print("Shape:", combined_df.shape)
display(combined_df.head(5))

# --- Step 2: Prepare the text data for embedding ---
# Make sure column name matches what you saved earlier
texts = combined_df['text'].astype(str).tolist()

print(f"Number of text entries to embed: {len(texts)}")

# --- Step 3: Load the SentenceTransformer model ---
# You can pick a smaller or larger model depending on speed/accuracy tradeoff
model = SentenceTransformer('all-MiniLM-L6-v2')

# --- Step 4: Create embeddings ---
embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)

print("Embeddings created!")
print("Shape of embeddings array:", embeddings.shape)

# --- Step 5: Save embeddings (for reuse or upload to Qdrant later) ---
import pickle

with open("plant_embeddings.pkl", "wb") as f:
    pickle.dump({'texts': texts, 'embeddings': embeddings}, f)

print("Embeddings and texts saved to 'plant_embeddings.pkl'")

"""3. Setting up Qdrant"""

from qdrant_client import QdrantClient

# Replace these with your actual credentials
QDRANT_URL = gardening_url 
QDRANT_API_KEY = gardening_api_key 

# Connect to your Qdrant cloud cluster
client = QdrantClient(
    url=QDRANT_URL,
    api_key=QDRANT_API_KEY
)

# Test connection
print("Connected to Qdrant!")
print("Collections available:", client.get_collections())

import pickle

# Load the file you saved earlier
with open("plant_embeddings.pkl", "rb") as f:
    data = pickle.load(f)

texts = data["texts"]
embeddings = data["embeddings"]

print("Embeddings loaded!")
print("Number of entries:", len(texts))
print("Embedding dimension:", embeddings.shape[1])

from qdrant_client.models import Distance, VectorParams

# Create a collection (name it something relevant)
collection_name = "qa_embeddings"

client.recreate_collection(
    collection_name=collection_name,
    vectors_config=VectorParams(size=384, distance=Distance.COSINE)
)

print(f"Collection '{collection_name}' created successfully!")

import pickle

# Load your pickle file (replace with your actual filename)
with open("plant_embeddings.pkl", "rb") as f:
    data = pickle.load(f)

# Check the type and a quick sample
print("Type:", type(data))

# If it's a dictionary, show the keys
if isinstance(data, dict):
    print("Keys:", data.keys())

# If it's a list, show the type of the first element
elif isinstance(data, list) and len(data) > 0:
    print("First element type:", type(data[0]))
    print("Sample:", data[0])

from qdrant_client.models import PointStruct
import uuid
import pickle
import numpy as np

# --- Load the pickle ---
with open("plant_embeddings.pkl", "rb") as f:
    data = pickle.load(f)

texts = data["texts"]
embeddings = np.array(data["embeddings"])

print(f"Loaded {len(texts)} entries with dimension {embeddings.shape[1]}")

# --- Confirm connection (assuming you already connected the client) ---
print("Collections available:", client.get_collections())

# --- Name of your existing collection ---
collection_name = "qa_embeddings"

# --- Prepare the points for upload ---
points = [
    PointStruct(
        id=str(uuid.uuid4()),  # unique ID per item
        vector=embeddings[i].tolist(),
        payload={"text": texts[i]}  # store the full QA text as metadata
    )
    for i in range(len(texts))
]

# --- Upload the data ---
client.upsert(collection_name=collection_name, points=points)

print(f"Uploaded {len(points)} embeddings to collection '{collection_name}'!")

collection_info = client.get_collection(collection_name)
print("Collection info:", collection_info)

query = "How does potatoes grow?"
query_vec = model.encode(query).tolist()

results = client.search(collection_name=collection_name, query_vector=query_vec, limit=3)
for r in results:
    print(f"Score: {r.score:.4f} | Text: {r.payload['text'][:150]}...")
